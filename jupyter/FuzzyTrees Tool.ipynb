{"cells":[{"cell_type":"code","source":"import pandas as _hex_pandas\nimport datetime as _hex_datetime\nimport json as _hex_json","execution_count":null,"metadata":{},"outputs":[]},{"cell_type":"code","source":"hex_scheduled = _hex_json.loads(\"false\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_user_email = _hex_json.loads(\"\\\"example-user@example.com\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_run_context = _hex_json.loads(\"\\\"logic\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_timezone = _hex_json.loads(\"\\\"UTC\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_project_id = _hex_json.loads(\"\\\"5f3b3a1d-8dd2-4270-810e-0c84aabdd482\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_project_name = _hex_json.loads(\"\\\"FuzzyTrees Tool\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_status = _hex_json.loads(\"\\\"\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_categories = _hex_json.loads(\"[]\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_color_palette = _hex_json.loads(\"[\\\"#4C78A8\\\",\\\"#F58518\\\",\\\"#E45756\\\",\\\"#72B7B2\\\",\\\"#54A24B\\\",\\\"#EECA3B\\\",\\\"#B279A2\\\",\\\"#FF9DA6\\\",\\\"#9D755D\\\",\\\"#BAB0AC\\\"]\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"<h4> Instructions (CSV) </h4>\n\nAbove, please input two CSV's:\n\n1. The first csv contains the practices you want to find matches for\n2. The second csv contains the larger set of practices you want to match to. \n\nBoth CSVs should be structured as below, with the columns in the following order: clinic id, name, city, address, and zip. It does not matter how you name the column headers, just as long as they are in the correct order of id, name, city, address, and zip for both CSVs.\n\n<hr>\n\n<img src=\"/api/v1/file/edaba88d-b1ef-41bd-a13f-c429ce776558\" width=\"500\"  />\n\n<hr>\n\n\n<!-- <h4> Instructions (Snowflake Tables) </h4> -->\n\n","metadata":{}},{"cell_type":"code","source":"table_name_1 = _hex_json.loads(\"\\\"\\\"\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"table_name_2 = _hex_json.loads(\"\\\"\\\"\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csv1 = pd.DataFrame()\ncsv2 = pd.DataFrame()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json as _hex_json\n_hex_pks.kernel_execution.input_cell.poll_filesystem(args=_hex_types.PollFilesystemArgs.from_dict({**_hex_json.loads(\"{\\\"filename\\\":\\\"6509ff26-7e97-40a5-9675-f03ca458d4aa.csv\\\",\\\"size\\\":309087,\\\"dir\\\":\\\"cell_uploads\\\"}\")}), app_session_token=_hex_APP_SESSION_TOKEN, python_kernel_init_status=_hex_python_kernel_init_status, hex_timezone=_hex_kernel.variable_or_none(\"hex_timezone\", scope_getter=lambda: globals()), interrupt_event=locals().get(\"_hex_interrupt_event\"))\nimport json as _hex_json\ncsv1 = _hex_pks.kernel_execution.input_cell.csv_to_dataframe(args=_hex_types.CSVToDataFrameArgs.from_dict({**_hex_json.loads(\"{\\\"filename\\\":\\\"6509ff26-7e97-40a5-9675-f03ca458d4aa.csv\\\",\\\"dir\\\":\\\"cell_uploads\\\"}\")}), app_session_token=_hex_APP_SESSION_TOKEN, python_kernel_init_status=_hex_python_kernel_init_status, hex_timezone=_hex_kernel.variable_or_none(\"hex_timezone\", scope_getter=lambda: globals()), interrupt_event=locals().get(\"_hex_interrupt_event\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json as _hex_json\n_hex_pks.kernel_execution.input_cell.poll_filesystem(args=_hex_types.PollFilesystemArgs.from_dict({**_hex_json.loads(\"{\\\"filename\\\":\\\"06822c81-f5a8-4889-bf41-a03439734a22.csv\\\",\\\"size\\\":1725177,\\\"dir\\\":\\\"cell_uploads\\\"}\")}), app_session_token=_hex_APP_SESSION_TOKEN, python_kernel_init_status=_hex_python_kernel_init_status, hex_timezone=_hex_kernel.variable_or_none(\"hex_timezone\", scope_getter=lambda: globals()), interrupt_event=locals().get(\"_hex_interrupt_event\"))\nimport json as _hex_json\ncsv2 = _hex_pks.kernel_execution.input_cell.csv_to_dataframe(args=_hex_types.CSVToDataFrameArgs.from_dict({**_hex_json.loads(\"{\\\"filename\\\":\\\"06822c81-f5a8-4889-bf41-a03439734a22.csv\\\",\\\"dir\\\":\\\"cell_uploads\\\"}\")}), app_session_token=_hex_APP_SESSION_TOKEN, python_kernel_init_status=_hex_python_kernel_init_status, hex_timezone=_hex_kernel.variable_or_none(\"hex_timezone\", scope_getter=lambda: globals()), interrupt_event=locals().get(\"_hex_interrupt_event\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fuzzy_match_now = _hex_json.loads(\"false\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!-- <img src=\"/api/v1/file/1b493483-e329-456f-94e9-5784854d003f\" width=\"900\" height=\"200\"  /> -->\n\n<img src=\"/api/v1/file/e3fe2587-3906-4e38-8f16-61aabb94e68b\"  width=\"300\" height=\"200\" />\n","metadata":{}},{"cell_type":"code","source":"if fuzzy_match_now: \n    csv1_original = csv1.copy(deep=True)\n    csv2_original = csv2.copy(deep=True) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if fuzzy_match_now: \n\n    csv1 = csv1.dropna()\n    csv2 = csv2.dropna()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if fuzzy_match_now: \n\n    csv1 = csv1.iloc[:,0:5]\n    columns = [\"id\",\"name\",\"city\",\"address\",\"zip\"]\n    # Renaming columns by index\n    new_column_names = {csv1.columns[i]: str(columns[i]) for i in range(len(csv1.columns))}\n    csv1 = csv1.rename(columns=new_column_names).reset_index(drop=True)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if fuzzy_match_now: \n\n    csv2 = csv2.iloc[:,0:5]\n    columns = [\"id2\",\"name2\",\"city2\",\"address2\",\"zip2\"]\n    # Renaming columns by index\n    new_column_names = {csv2.columns[i]: str(columns[i]) for i in range(len(csv2.columns))}\n    csv2 = csv2.rename(columns=new_column_names).reset_index(drop=True)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to check if a float has all decimal places as .0\ndef is_decimal_float(value):\n    return value.is_integer()\n\ndef update_table(df):\n# Iterate over columns\n    for col in df.columns:\n        # Check if all values in the column are float and have decimal places as .0\n        if df[col].dtype == 'float64' and all(is_decimal_float(value) for value in df[col]):\n            # Convert the column to integer\n            df[col] = df[col].astype(int)\n\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if fuzzy_match_now: \n\n    csv1 = update_table(csv1)\n    csv2 = update_table(csv2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if train_random_forest:\n!pip install jellyfish\nimport jellyfish as j","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"Requirement already satisfied: jellyfish in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (1.0.3)\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt \nimport numpy as np \nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc\nimport fuzzywuzzy as f\nfrom fuzzywuzzy import fuzz\nfrom joblib import dump, load\nimport time\nimport sys\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if fuzzy_match_now: \n\n    # Load the model from the file\n    random_forest_classifier = load('random_forest_model.joblib')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jaro_winkler_distance(str1,str2):\n    score = j.jaro_winkler_similarity(str1,str2)\n    return score\n\ndef levenshtein_distance(str1,str2):\n    score = f.fuzz.ratio(str1, str2)/100\n    return score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef generate_feature_vectors(columns_features,pair_data,distance_function):\n    for jj in range(len(columns_features)):\n        # print(\"Creating feature vector for: \" + columns_features[jj])\n        distances = np.zeros((len(pair_data),1))\n        for ii in range(len(pair_data)):\n            distance = distance_function(str(pair_data.iloc[ii,jj+1]),str(pair_data.iloc[ii,jj+6]))\n            distances[ii] = (distance)\n        pair_data[columns_features[jj]] = distances\n    return pair_data\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lev_features = ['name_lev_dist','city_lev_dist','address_lev_dist','zip_lev_dist'] \njaro_features = ['name_jaro_dist','city_jaro_dist','address_jaro_dist','zip_jaro_dist']\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"match_table = pd.DataFrame()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndef update_loading_bar(current, total):\n    \"\"\"\n    Updates the loading bar in the console.\n\n    Parameters:\n    - current: the current index (current progress).\n    - total: the total number of iterations (total progress to be made).\n\n    The function calculates the percentage of completion and updates the loading bar accordingly.\n    \"\"\"\n    bar_length = 50  # You can adjust the length of the loading bar here\n    progress = float(current) / total\n    block = int(round(bar_length * progress))\n    bar = \"\\rProgress: [{0}] {1}%\".format(\"#\" * block + \"-\" * (bar_length - block), round(progress * 100, 2))\n    sys.stdout.write(bar)\n    sys.stdout.flush()\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if fuzzy_match_now:\n    print(\"Running FuzzyTrees..\")","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"Running FuzzyTrees..\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"\nif fuzzy_match_now:\n    print(\"Running FuzzyTrees..\")\n    start_time = time.time()  # capture start time\n    total_vso_ids = len(csv2)\n    match_table = pd.DataFrame(np.zeros((len(csv1),11)),columns=['source_1_id', 'source_1_name', 'source_1_city', 'source_1_address', 'source_1_zip', 'source_2_id', 'source_2_name', 'source_2_city', 'source_2_address', 'source_2_zip','total_score'])\n    table_idx = 0\n    # Specify data types\n    data_types = {\n        'id': int,\n        'name': object,\n        'city': object,\n        'address': object,\n        'zip': int\n    }\n\n    for ii in range(len(csv1)):\n        update_loading_bar(ii, len(csv1)-1)\n        tmp_csv1_df = csv1.iloc[ii,:]\n        # Repeat the row N times\n        duplicates = pd.concat([tmp_csv1_df]*total_vso_ids, axis=1).transpose()\n        # Append duplicates to the original DataFrame\n        tmp_csv1_df = pd.concat([duplicates], ignore_index=True)\n\n        tmp_csv1_df = tmp_csv1_df.astype(data_types)\n        tmp_merged_df_original = pd.concat([tmp_csv1_df,csv2.reset_index(drop=True)],axis=1)\n        tmp_merged_df = tmp_merged_df_original.copy(deep=True)\n\n        tmp_merged_df.loc[:, tmp_merged_df.dtypes == 'object'] = tmp_merged_df.select_dtypes(['object']).apply(lambda x: x.str.lower())\n        tmp_merged_df.loc[:, tmp_merged_df.dtypes == 'object'] = tmp_merged_df.select_dtypes(['object']).apply(lambda x: x.str.replace('[ ,-]', '', regex=True))\n        tmp_merged_df.loc[:, tmp_merged_df.dtypes == 'object'] = tmp_merged_df.select_dtypes(['object']).apply(lambda x: x.str.replace('[&]', 'and', regex=True))\n\n\n        lev_features = ['name_lev_dist','city_lev_dist','address_lev_dist','zip_lev_dist'] \n        jaro_features = ['name_jaro_dist','city_jaro_dist','address_jaro_dist','zip_jaro_dist']\n        tmp_merged_df = generate_feature_vectors(lev_features,tmp_merged_df,levenshtein_distance)\n        tmp_merged_df = generate_feature_vectors(jaro_features,tmp_merged_df,jaro_winkler_distance)\n        columns_features = ['name_lev_dist','city_lev_dist','address_lev_dist','zip_lev_dist','name_jaro_dist','city_jaro_dist','address_jaro_dist','zip_jaro_dist']\n        tmp_merged_df['total_score'] = tmp_merged_df.loc[:,columns_features].sum(axis=1)\n        columns_features = ['name_lev_dist','city_lev_dist','address_lev_dist','zip_lev_dist','name_jaro_dist','city_jaro_dist','address_jaro_dist','zip_jaro_dist','total_score']\n\n        feature_df = tmp_merged_df.loc[:,columns_features]\n        X = np.array(feature_df)  # Features\n\n        y_pred = random_forest_classifier.predict(X)\n\n        indices = np.where(y_pred == 1)[0]\n        if len(indices) != 0:\n            tmp_df2 = tmp_merged_df.loc[indices,columns_features]\n            tmp_df2['summed'] = tmp_df2.sum(axis=1)\n            max_index = tmp_df2['summed'].idxmax()\n\n            match_table.iloc[table_idx,0:10] = tmp_merged_df_original.iloc[int(max_index),0:10]\n            match_table.iloc[table_idx,10] = tmp_merged_df.loc[int(max_index),'total_score']\n            table_idx += 1    \n        else:\n            pass  \n\n    match_table = match_table.loc[~(match_table==0).all(axis=1)]\n    end_time = time.time()  # capture end time\n    elapsed_time_seconds = end_time - start_time  # total time in seconds\n    elapsed_time_minutes = elapsed_time_seconds / 60  # convert seconds to minutes\n\n    print(f\"\\n Elapsed time: {np.round(elapsed_time_minutes,2)} minutes\")\n#     # match_table = match_table[match_table['total_score']>7]\n    match_table = update_table(match_table)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"match_table","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_table = pd.DataFrame()\n\n\nif fuzzy_match_now: \n\n    list(csv2_original.columns)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if fuzzy_match_now: \n\n    total_cols = np.size(csv1_original,1) + np.size(csv2_original,1)\n    output_table = pd.DataFrame(np.zeros((len(match_table),total_cols)),columns=list(csv1_original.columns) + list(csv2_original.columns))\n\n    for ii in range(len(match_table)):\n        print(ii)\n        tmp1 = list(csv1_original[csv1_original.iloc[:,0] == match_table.source_1_id[ii]].iloc[0,:])\n        tmp2 = list(csv2_original[csv2_original.iloc[:,0] == match_table.source_2_id[ii]].iloc[0,:])\n        tmp_concat = tmp1 + tmp2\n        output_table.iloc[ii,:] = tmp_concat","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The table below has additional columns preserved:","metadata":{}},{"cell_type":"code","source":"output_table","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if 1:\n#     print(\"Running FuzzyTrees..\")\n#     start_time = time.time()  # capture start time\n#     total_vso_ids = len(csv2)\n#     match_table = pd.DataFrame(np.zeros((len(csv1),11)),columns=['source_1_id', 'source_1_name', 'source_1_city', 'source_1_address', 'source_1_zip', 'source_2_id', 'source_2_name', 'source_2_city', 'source_2_address', 'source_2_zip','total_score'])\n#     table_idx = 0\n#     # Specify data types\n#     data_types = {\n#         'id': int,\n#         'name': object,\n#         'city': object,\n#         'address': object,\n#         'zip': int,\n#         # 'id2': int,\n#         # 'name2': object,\n#         # 'city2': object,\n#         # 'address2': object,\n#         # 'zip2': int\n#     }\n\n#     for ii in range(1):\n#         update_loading_bar(ii, len(csv1)-1)\n#         tmp_csv1_df = csv1.iloc[ii,:]\n#         # Repeat the row N times\n#         duplicates = pd.concat([tmp_csv1_df]*total_vso_ids, axis=1).transpose()\n#         # Append duplicates to the original DataFrame\n#         tmp_csv1_df = pd.concat([duplicates], ignore_index=True)\n\n#         tmp_csv1_df = tmp_csv1_df.astype(data_types)\n#         tmp_merged_df_original = pd.concat([tmp_csv1_df,csv2.reset_index(drop=True)],axis=1)\n#         tmp_merged_df = tmp_merged_df_original.copy(deep=True)\n#         # tmp_merged_df = tmp_merged_df.astype(data_types)\n\n#         tmp_merged_df.loc[:, tmp_merged_df.dtypes == 'object'] = tmp_merged_df.select_dtypes(['object']).apply(lambda x: x.str.lower())\n#         tmp_merged_df.loc[:, tmp_merged_df.dtypes == 'object'] = tmp_merged_df.select_dtypes(['object']).apply(lambda x: x.str.replace('[ ,-]', '', regex=True))\n#         tmp_merged_df.loc[:, tmp_merged_df.dtypes == 'object'] = tmp_merged_df.select_dtypes(['object']).apply(lambda x: x.str.replace('[&]', 'and', regex=True))\n\n\n#         lev_features = ['name_lev_dist','city_lev_dist','address_lev_dist','zip_lev_dist'] \n#         jaro_features = ['name_jaro_dist','city_jaro_dist','address_jaro_dist','zip_jaro_dist']\n#         tmp_merged_df = generate_feature_vectors(lev_features,tmp_merged_df,levenshtein_distance)\n#         tmp_merged_df = generate_feature_vectors(jaro_features,tmp_merged_df,jaro_winkler_distance)\n#         columns_features = ['name_lev_dist','city_lev_dist','address_lev_dist','zip_lev_dist','name_jaro_dist','city_jaro_dist','address_jaro_dist','zip_jaro_dist']\n#         tmp_merged_df['total_score'] = tmp_merged_df.loc[:,columns_features].sum(axis=1)\n#         columns_features = ['name_lev_dist','city_lev_dist','address_lev_dist','zip_lev_dist','name_jaro_dist','city_jaro_dist','address_jaro_dist','zip_jaro_dist','total_score']\n\n#         feature_df = tmp_merged_df.loc[:,columns_features]\n#         X = np.array(feature_df)  # Features\n\n#         y_pred = random_forest_classifier.predict(X)\n\n#         indices = np.where(y_pred == 1)[0]\n#         if len(indices) != 0:\n#             tmp_df2 = tmp_merged_df.loc[indices,columns_features]\n#             tmp_df2['summed'] = tmp_df2.sum(axis=1)\n#             max_index = tmp_df2['summed'].idxmax()\n\n#             match_table.iloc[table_idx,0:10] = tmp_merged_df_original.iloc[int(max_index),0:10]\n#             match_table.iloc[table_idx,10] = tmp_merged_df.loc[int(max_index),'total_score']\n#             table_idx += 1    \n#         else:\n#             pass  \n\n#     match_table = match_table.loc[~(match_table==0).all(axis=1)]\n#     end_time = time.time()  # capture end time\n#     elapsed_time_seconds = end_time - start_time  # total time in seconds\n#     elapsed_time_minutes = elapsed_time_seconds / 60  # convert seconds to minutes\n\n#     print(f\"\\n Elapsed time: {np.round(elapsed_time_minutes,2)} minutes\")\n# #     # match_table = match_table[match_table['total_score']>7]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tmp_merged_df.sort_values(by='total_score',ascending=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# update_loading_bar(ii, len(csv1)-1)\n# tmp_csv1_df = csv1.iloc[ii,:]\n# # Repeat the row N times\n# duplicates = pd.concat([tmp_csv1_df]*total_vso_ids, axis=1).transpose()\n# # Append duplicates to the original DataFrame\n# tmp_csv1_df = pd.concat([duplicates], ignore_index=True)\n\n# # tmp_csv1_df = tmp_csv1_df.astype(data_types)\n# tmp_merged_df_original = pd.concat([tmp_csv1_df,csv2.reset_index(drop=True)],axis=1)\n# tmp_merged_df = tmp_merged_df_original.copy(deep=True)\n# # tmp_merged_df = tmp_merged_df.astype(data_types)\n# tmp_merged_df\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# csv2","metadata":{},"execution_count":null,"outputs":[]}],"metadata":{"orig_nbformat":4,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"hex_info":{"author":"James Graham","project_id":"5f3b3a1d-8dd2-4270-810e-0c84aabdd482","version":"draft","exported_date":"Sun Jul 28 2024 18:08:47 GMT+0000 (Coordinated Universal Time)"}},"nbformat":4,"nbformat_minor":4}